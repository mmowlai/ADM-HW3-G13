{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from heapq import heappush, nlargest\n",
    "import heapq\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "from time import time \n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import math\n",
    "import json\n",
    "import _pickle as pickle\n",
    "import ast\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of movies inorder to get the web pages\n",
    "movie_list = []\n",
    "urls = ['https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies1.html',\n",
    "        'https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies2.html',\n",
    "        'https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies3.html']\n",
    "       \n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        movie_list.append(link['href'])\n",
    "        with open('movie_list.txt', 'a') as f:\n",
    "            f.write(link['href'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting all the WebPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in range(len(movie_list)):\n",
    "    try:\n",
    "        response = requests.get(movie_list[movie])\n",
    "    except:\n",
    "        time.sleep(1201)  # 20 mins of sleep in case of blocking by wikipedia\n",
    "        response = requests.get(movie_list[movie])\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    name = \"article-\"+ str(movie) +\".html\"\n",
    "    page = str(soup)\n",
    "    time.sleep(1)\n",
    "    with open(name, 'a') as f:\n",
    "        f.write(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here at path we have all the html files\n",
    "path = './0-30k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('tsv_files'):\n",
    "    os.makedirs('tsv_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want just to parse the intro paragraphs and plot paragraphs\n",
    "def intro_plot(soup,index=0):\n",
    "    try:\n",
    "        Node = soup.find_all('p')[index]\n",
    "        lst = [Node.text]\n",
    "        nextNode = Node\n",
    "        while True:\n",
    "            nextNode = nextNode.next_sibling\n",
    "\n",
    "            try:\n",
    "                tag_name = nextNode.name\n",
    "            except AttributeError:\n",
    "                tag_name = \"\"\n",
    "            if tag_name == \"p\":\n",
    "                lst.append(nextNode.text)\n",
    "            else:\n",
    "                break\n",
    "    except:\n",
    "        lst = None\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the desire informations\n",
    "def soup_parser(soup,info=None,next_tag='td',class_name=None):\n",
    "    if next_tag:\n",
    "        element = soup.find('th', text=info)\n",
    "        if not element:\n",
    "            element = 'NA'\n",
    "        else:\n",
    "            if info=='Release date':\n",
    "                element = element.find_next(next_tag).text.split()[0]\n",
    "            else:\n",
    "                element = element.find_next(next_tag).text\n",
    "    else:\n",
    "        element = soup.find('h1').text\n",
    "    return element\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the all tsv files\n",
    "columns = ['title', 'intro', 'plot', 'film_name', 'director', 'producer', 'writer', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget']\n",
    "exceptions = []\n",
    "start = time()\n",
    "for subdir, dirs,files in os.walk(path):\n",
    "    for file in files:\n",
    "        try:\n",
    "            soup = BS(open(path+file, encoding=\"utf8\"), \"html.parser\")\n",
    "            title = soup_parser(soup=soup,next_tag=None,class_name='firstHeading')\n",
    "            intro = intro_plot(soup=soup)\n",
    "            plot = intro_plot(soup=soup,index=len(intro))\n",
    "            index = 0\n",
    "            while intro == ['\\n']:\n",
    "                index+=1\n",
    "                intro = intro_plot(soup=soup,index=index)\n",
    "                plot = intro_plot(soup=soup,index=len(intro)+index)\n",
    "            director = soup_parser(soup=soup, info='Directed by')\n",
    "            producer = soup_parser(soup=soup, info='Produced by')\n",
    "            writer = soup_parser(soup=soup, info='Written by')\n",
    "            starred = soup_parser(soup=soup, info='Starring')\n",
    "            music = soup_parser(soup=soup, info='Music by')\n",
    "            date = soup_parser(soup=soup,info='Release date')\n",
    "            duration = soup_parser(soup=soup, info='Running time')\n",
    "            country = soup_parser(soup=soup, info='Country')\n",
    "            language = soup_parser(soup=soup, info='Language')\n",
    "            budget = soup_parser(soup=soup, info='Budget')\n",
    "            \n",
    "            all_info = [title,intro,plot,title,director,producer,writer,starred,music,date,duration,country,language,budget]\n",
    "            with open('tsv_files/%s.tsv' %(file[:-5]), 'wt',encoding=\"utf-8\") as out_file:\n",
    "                tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "                tsv_writer.writerow(columns)\n",
    "                tsv_writer.writerow(all_info)\n",
    "\n",
    "        except:\n",
    "            print('Could not parse html file:', file)\n",
    "            exceptions.append(file)\n",
    "print(\"Execution time is:\",time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "def f_clean(i):\n",
    "    s_word = stopwords.words('english')\n",
    "    i = i.replace(\"\\\\n\",\"\")\n",
    "    i = i.lower()\n",
    "    i = i.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    word = word_tokenize(i) \n",
    "    \n",
    "    filt = [w for w in word if not w in s_word]\n",
    "    ps = PorterStemmer()\n",
    "    stemmed = []\n",
    "    for w in filt:\n",
    "        stemmed.append(ps.stem(w))\n",
    "    \n",
    "    punctuation = list(string.punctuation)\n",
    "    punctuation.append(\"''\")\n",
    "    \n",
    "    without_punt = [w for w in stemmed if not w in punctuation]\n",
    "\n",
    "    return without_punt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of the links in movies inorder to make a dataframe of films\n",
    "tree1 = html.parse(r\"movies1.html\")\n",
    "movies1 = html.tostring(tree1)\n",
    "soup1 = BS(movies1)\n",
    "\n",
    "tree2 = html.parse(r\"movies2.html\")\n",
    "movies2 = html.tostring(tree2)\n",
    "soup2 = BS(movies2)\n",
    "\n",
    "tree3 = html.parse(r\"movies3.html\")\n",
    "movies3 = html.tostring(tree3)\n",
    "soup3 = BS(movies3)\n",
    "\n",
    "links = []\n",
    "for link in soup1.findAll('a'):\n",
    "    links.append(link.get('href'))\n",
    "for link in soup2.findAll('a'):\n",
    "    links.append(link.get('href'))\n",
    "for link in soup3.findAll('a'):\n",
    "    links.append(link.get('href'))\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame of all films with their links\n",
    "c = ['url','title','intro','plot','film_name','director','producer','writer','starring','music',\n",
    "           'release_date','runtime','country','language','budget']\n",
    "df_films = pd.DataFrame(columns = c)\n",
    "for i in range(30000):\n",
    "    with open(\"tsv_files/article-{}.tsv\".format(i)) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        l = list(reader)[1]\n",
    "        r = [links[i]] + l[0:]\n",
    "        df_films = df_films.append(pd.DataFrame([r],columns = c),ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to a csv file\n",
    "df_films.to_csv('film.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_film = pd.DataFrame(pd.read_csv('film.csv'))   # in the film.csv we have a data of all films\n",
    "df_film.rename(columns = {'Unnamed: 0':'film_id'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vocabulary and set a new column in the dataframe with the len on the plot+intro\n",
    "def create_voc(row,vocabulary):\n",
    "    film_id = row['film_id']\n",
    "    try:\n",
    "        text = f_clean(row['intro']+row['plot'])\n",
    "    except Exception as e:\n",
    "        print(film_id,e)\n",
    "        return 0\n",
    "    for w in text:\n",
    "        if w not in vocabulary:\n",
    "            vocabulary[w] = {film_id:1}\n",
    "        else:\n",
    "            if film_id not in vocabulary[w]:\n",
    "                vocabulary[w][film_id] = 1\n",
    "            else:\n",
    "                vocabulary[w][film_id] += 1\n",
    "    return len(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_film['plot'] = df_film['plot'].astype(str)\n",
    "v = {}\n",
    "df_film['len_text'] = df_film.apply(create_voc,axis=1, vocabulary = v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we map the keys of the vocabulary in a number\n",
    "def map_voc(voc):       \n",
    "    i = 0 \n",
    "    new_voc = {}\n",
    "    for e in voc.keys():\n",
    "        new_voc[e] = i\n",
    "        i +=1\n",
    "    return new_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " mp = map_voc(v) #map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "json.dump(mp, open(\"map.txt\",'w'))\n",
    "# Store data (serialize)\n",
    "json.dump(v, open(\"voc.txt\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "mp = json.load(open(\"map.txt\"))\n",
    "voca = json.load(open(\"voc.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first search engine\n",
    "def query1():\n",
    "    a = input(\"Enter a Query: \")\n",
    "    a = f_clean(a)     #we need to clean the input so we have the match between the word\n",
    "    l = []\n",
    "    for i in a:\n",
    "        try:\n",
    "            l1 = list(voca[i].keys())\n",
    "            l += [l1]\n",
    "        except Exception as e:\n",
    "            print(\"Not in any films:\"+str(a))\n",
    "            return\n",
    "    e = list(reduce(set.intersection, [set(item) for item in l ]))  # we find the intersection from all the lis\n",
    "    p = df_film[df_film['film_id'].isin(e)]  #we select only the film that are in the intesection\n",
    "    p.reset_index(inplace=True)\n",
    "    return p[['title','intro','url']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a Query: love\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Arcadian Maid</td>\n",
       "      <td>['An Arcadian Maid is a 1910 American silent f...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/An_Arcadian_Maid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pocahontas (1910 film)</td>\n",
       "      <td>[\"Pocahontas is a 1910 American silent short d...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Pocahontas_(1910...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ramona (1910 film)</td>\n",
       "      <td>[\"Ramona is a 1910 American short drama film d...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ramona_(1910_film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What the Daisy Said</td>\n",
       "      <td>['What the Daisy Said is a one-reel film (abou...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/What_the_Daisy_Said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brown of Harvard (1911 film)</td>\n",
       "      <td>['Brown of Harvard is a 1911 silent film based...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Brown_of_Harvard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>For Her Sake</td>\n",
       "      <td>[\"For Her Sake is a 1911 American silent short...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/For_Her_Sake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Put Yourself in His Place</td>\n",
       "      <td>['Put Yourself in His Place is a 1912 American...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Put_Yourself_in_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Atlantis (1913 film)</td>\n",
       "      <td>['Atlantis is a 1913 Danish silent film direct...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Atlantis_(1913_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Quakeress</td>\n",
       "      <td>['The Quakeress is a 1913 silent era short cos...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Quakeress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Avenging Conscience</td>\n",
       "      <td>['The Avenging Conscience: or \"Thou Shalt Not ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Avenging_Con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title  \\\n",
       "0              An Arcadian Maid   \n",
       "1        Pocahontas (1910 film)   \n",
       "2            Ramona (1910 film)   \n",
       "3           What the Daisy Said   \n",
       "4  Brown of Harvard (1911 film)   \n",
       "5                  For Her Sake   \n",
       "6     Put Yourself in His Place   \n",
       "7          Atlantis (1913 film)   \n",
       "8                 The Quakeress   \n",
       "9       The Avenging Conscience   \n",
       "\n",
       "                                               intro  \\\n",
       "0  ['An Arcadian Maid is a 1910 American silent f...   \n",
       "1  [\"Pocahontas is a 1910 American silent short d...   \n",
       "2  [\"Ramona is a 1910 American short drama film d...   \n",
       "3  ['What the Daisy Said is a one-reel film (abou...   \n",
       "4  ['Brown of Harvard is a 1911 silent film based...   \n",
       "5  [\"For Her Sake is a 1911 American silent short...   \n",
       "6  ['Put Yourself in His Place is a 1912 American...   \n",
       "7  ['Atlantis is a 1913 Danish silent film direct...   \n",
       "8  ['The Quakeress is a 1913 silent era short cos...   \n",
       "9  ['The Avenging Conscience: or \"Thou Shalt Not ...   \n",
       "\n",
       "                                                 url  \n",
       "0     https://en.wikipedia.org/wiki/An_Arcadian_Maid  \n",
       "1  https://en.wikipedia.org/wiki/Pocahontas_(1910...  \n",
       "2   https://en.wikipedia.org/wiki/Ramona_(1910_film)  \n",
       "3  https://en.wikipedia.org/wiki/What_the_Daisy_Said  \n",
       "4  https://en.wikipedia.org/wiki/Brown_of_Harvard...  \n",
       "5         https://en.wikipedia.org/wiki/For_Her_Sake  \n",
       "6  https://en.wikipedia.org/wiki/Put_Yourself_in_...  \n",
       "7  https://en.wikipedia.org/wiki/Atlantis_(1913_f...  \n",
       "8        https://en.wikipedia.org/wiki/The_Quakeress  \n",
       "9  https://en.wikipedia.org/wiki/The_Avenging_Con...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define a function to calculates the tf idf\n",
    "def invertx_voc(voc):\n",
    "    new_voc = {}\n",
    "    for k in voc.keys():\n",
    "        repetition = len(voc[k])\n",
    "        IDF = math.log(30000/repetition)\n",
    "        for elem in voc[k].keys():\n",
    "            val = voc[k][elem]\n",
    "            length = list(df_film[df_film['film_id'] == int(elem)]['len_text'])[0]\n",
    "            \n",
    "            tf = val/length\n",
    "            \n",
    "            if k not in new_voc:\n",
    "                new_voc[k] = {elem : tf*IDF}\n",
    "            else:\n",
    "                new_voc[k][elem] = tf*IDF\n",
    "    return new_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfldf = invertx_voc(voca)\n",
    "json.dump(tfldf, open(\"tfidf.txt\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfldf = json.load(open(\"tfidf.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second search engine with cosine similarity\n",
    "def query2():\n",
    "    def cosSim(row):\n",
    "        film_vector = []\n",
    "        for elem in cleaned_query:\n",
    "            if(elem in tfldf):\n",
    "                if(str(row['film_id']) in tfldf[elem]):\n",
    "                    film_vector.append(tfldf[elem][str(row['film_id'])])\n",
    "                else:\n",
    "                    film_vector.append(0)\n",
    "            else:\n",
    "                film_vector.append(0)\n",
    "        query_vector_idf = tfidf_query(cleaned_query)\n",
    "        cos_sim = cosine_similarity([film_vector], [query_vector_idf])[0][0]\n",
    "        return cos_sim\n",
    "    def tfidf_query(q):\n",
    "        tfidf_q = []\n",
    "        for elem in q:\n",
    "            tfidf_q.append(1)\n",
    "        return tfidf_q\n",
    "\n",
    "    query = input(\"Enter a Query: \")\n",
    "    tfldf = json.load(open(\"tfidf.txt\"))\n",
    "    cleaned_query = f_clean(query)\n",
    "    query_vector = tfidf_query(cleaned_query)\n",
    "    df_film['Similarity'] = df_film.apply(cosSim, axis = 1)\n",
    "    ndf = df_film[['title', 'intro','url', 'Similarity']]\n",
    "    result = ndf[ndf['Similarity'] > 0.7].sort_values('Similarity', ascending = False).head(10)\n",
    "    return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a Query: love\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>An Arcadian Maid</td>\n",
       "      <td>['An Arcadian Maid is a 1910 American silent f...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/An_Arcadian_Maid</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23472</th>\n",
       "      <td>Halik sa Hangin</td>\n",
       "      <td>[\"Halik sa Hangin (lit.\\u2009Kiss in the Wind)...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Halik_sa_Hangin</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23477</th>\n",
       "      <td>The Love Affair (film)</td>\n",
       "      <td>['The Love Affair is a 2015 Filipino drama fil...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Love_Affair_...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23479</th>\n",
       "      <td>Ex with Benefits</td>\n",
       "      <td>['Ex with Benefits is a 2015 Filipino romantic...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ex_with_Benefits</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23482</th>\n",
       "      <td>Etiquette for Mistresses</td>\n",
       "      <td>['Etiquette for Mistresses [3] is a 2015 Phili...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Etiquette_for_Mi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23483</th>\n",
       "      <td>The Prenup</td>\n",
       "      <td>['The Prenup is a 2015 Filipino romantic comed...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Prenup</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23484</th>\n",
       "      <td>Everyday I Love You (film)</td>\n",
       "      <td>['Everyday, I Love You is a 2015 Filipino roma...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Everyday_I_Love_...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23486</th>\n",
       "      <td>Love Is Blind (2016 film)</td>\n",
       "      <td>['Love is Blind is a 2016 Philippine fantasy-r...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_is_Blind_(2...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23487</th>\n",
       "      <td>Padre de Familia (film)</td>\n",
       "      <td>['Padre de Familia is a 2016 independent famil...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Padre_de_Familia...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23490</th>\n",
       "      <td>Love Me Tomorrow (film)</td>\n",
       "      <td>['Love Me Tomorrow is a 2016 Filipino romantic...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_Me_Tomorrow...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title  \\\n",
       "25               An Arcadian Maid   \n",
       "23472             Halik sa Hangin   \n",
       "23477      The Love Affair (film)   \n",
       "23479            Ex with Benefits   \n",
       "23482    Etiquette for Mistresses   \n",
       "23483                  The Prenup   \n",
       "23484  Everyday I Love You (film)   \n",
       "23486   Love Is Blind (2016 film)   \n",
       "23487     Padre de Familia (film)   \n",
       "23490     Love Me Tomorrow (film)   \n",
       "\n",
       "                                                   intro  \\\n",
       "25     ['An Arcadian Maid is a 1910 American silent f...   \n",
       "23472  [\"Halik sa Hangin (lit.\\u2009Kiss in the Wind)...   \n",
       "23477  ['The Love Affair is a 2015 Filipino drama fil...   \n",
       "23479  ['Ex with Benefits is a 2015 Filipino romantic...   \n",
       "23482  ['Etiquette for Mistresses [3] is a 2015 Phili...   \n",
       "23483  ['The Prenup is a 2015 Filipino romantic comed...   \n",
       "23484  ['Everyday, I Love You is a 2015 Filipino roma...   \n",
       "23486  ['Love is Blind is a 2016 Philippine fantasy-r...   \n",
       "23487  ['Padre de Familia is a 2016 independent famil...   \n",
       "23490  ['Love Me Tomorrow is a 2016 Filipino romantic...   \n",
       "\n",
       "                                                     url  Similarity  \n",
       "25        https://en.wikipedia.org/wiki/An_Arcadian_Maid         1.0  \n",
       "23472      https://en.wikipedia.org/wiki/Halik_sa_Hangin         1.0  \n",
       "23477  https://en.wikipedia.org/wiki/The_Love_Affair_...         1.0  \n",
       "23479     https://en.wikipedia.org/wiki/Ex_with_Benefits         1.0  \n",
       "23482  https://en.wikipedia.org/wiki/Etiquette_for_Mi...         1.0  \n",
       "23483           https://en.wikipedia.org/wiki/The_Prenup         1.0  \n",
       "23484  https://en.wikipedia.org/wiki/Everyday_I_Love_...         1.0  \n",
       "23486  https://en.wikipedia.org/wiki/Love_is_Blind_(2...         1.0  \n",
       "23487  https://en.wikipedia.org/wiki/Padre_de_Familia...         1.0  \n",
       "23490  https://en.wikipedia.org/wiki/Love_Me_Tomorrow...         1.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third search engine with the most similarity of desired runtime and the film's runtime\n",
    "def query3():\n",
    "    q_min = input(\"DESIRE RUNTIME(in mins): \")\n",
    "    inp = input(\"Enter The Query: \")\n",
    "    inp = f_clean(inp)     #we need to clean the input\n",
    "    l = []\n",
    "    for i in inp:\n",
    "        try:\n",
    "            l1 = list(voca[i].keys())\n",
    "            l += [l1]\n",
    "        except Exception as e:\n",
    "            print(\"Not in any films:\"+str(inp))\n",
    "            return\n",
    "    e = list(reduce(set.intersection, [set(item) for item in l ]))  # we find the intersection from all the lis\n",
    "    p = df_film[df_film['film_id'].isin(e)]  #we select only the film that are in the intesection\n",
    "    p.reset_index(inplace=True)\n",
    "    p = p[['title','intro','url','runtime']]\n",
    "    sim = []\n",
    "    # here we calculate sort-scoring\n",
    "    for i in range(len(p.runtime)):\n",
    "        try:\n",
    "            resrun = int(p.runtime[i].split('minutes')[0].strip())\n",
    "            qrun   = int(q_min.strip())\n",
    "            calc   = abs(resrun - qrun)\n",
    "            sim.append(calc)\n",
    "        except:\n",
    "            # in some fields that we don't have any data for runtimes we replace it by a very large number\n",
    "            sim.append(int('1000000')) \n",
    "    p['Differrence'] = sim   \n",
    "    p.sort_values('Differrence',ascending = True,inplace = True)\n",
    "    return p.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESIRE RUNTIME(in mins): 60\n",
      "Enter The Query: love\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "      <th>runtime</th>\n",
       "      <th>Differrence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>The Case of the Black Parrot</td>\n",
       "      <td>['The Case of the Black Parrot is a 1941 Ameri...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Case_of_the_...</td>\n",
       "      <td>60 minutes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>His Private Secretary</td>\n",
       "      <td>['His Private Secretary is a 1933 American Pre...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/His_Private_Secr...</td>\n",
       "      <td>60 minutes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Woman's Place</td>\n",
       "      <td>[\"Woman's Place is a 1921 American romantic co...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Woman%27s_Place</td>\n",
       "      <td>60 minutes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>Headline Shooter</td>\n",
       "      <td>['Headline Shooter is a 1933 American pre-Code...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Headline_Shooter</td>\n",
       "      <td>60 minutes[3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>Escape to Paradise</td>\n",
       "      <td>['Escape to Paradise is a 1939 American film d...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Escape_to_Paradise</td>\n",
       "      <td>60 minutes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Love Never Dies (1921 film)</td>\n",
       "      <td>['Love Never Dies is a 1921 American silent dr...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_Never_Dies_...</td>\n",
       "      <td>60 minutes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>The Blonde Bandit</td>\n",
       "      <td>['The Blonde Bandit is a 1950 American crime f...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Blonde_Bandit</td>\n",
       "      <td>60 minutes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Heedless Moths</td>\n",
       "      <td>['Heedless Moths is a 1921 American silent mel...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Heedless_Moths</td>\n",
       "      <td>60 minutes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766</th>\n",
       "      <td>All at Sea (1935 film)</td>\n",
       "      <td>['All at Sea is a 1935 British comedy film dir...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/All_at_Sea_(1935...</td>\n",
       "      <td>60 minutes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>A Splendid Hazard</td>\n",
       "      <td>['A Splendid Hazard is a 1920 American silent ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/A_Splendid_Hazar...</td>\n",
       "      <td>60 minutes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "1274  The Case of the Black Parrot   \n",
       "722          His Private Secretary   \n",
       "169                  Woman's Place   \n",
       "720               Headline Shooter   \n",
       "1118            Escape to Paradise   \n",
       "158    Love Never Dies (1921 film)   \n",
       "1961             The Blonde Bandit   \n",
       "153                 Heedless Moths   \n",
       "5766        All at Sea (1935 film)   \n",
       "143              A Splendid Hazard   \n",
       "\n",
       "                                                  intro  \\\n",
       "1274  ['The Case of the Black Parrot is a 1941 Ameri...   \n",
       "722   ['His Private Secretary is a 1933 American Pre...   \n",
       "169   [\"Woman's Place is a 1921 American romantic co...   \n",
       "720   ['Headline Shooter is a 1933 American pre-Code...   \n",
       "1118  ['Escape to Paradise is a 1939 American film d...   \n",
       "158   ['Love Never Dies is a 1921 American silent dr...   \n",
       "1961  ['The Blonde Bandit is a 1950 American crime f...   \n",
       "153   ['Heedless Moths is a 1921 American silent mel...   \n",
       "5766  ['All at Sea is a 1935 British comedy film dir...   \n",
       "143   ['A Splendid Hazard is a 1920 American silent ...   \n",
       "\n",
       "                                                    url        runtime  \\\n",
       "1274  https://en.wikipedia.org/wiki/The_Case_of_the_...     60 minutes   \n",
       "722   https://en.wikipedia.org/wiki/His_Private_Secr...     60 minutes   \n",
       "169       https://en.wikipedia.org/wiki/Woman%27s_Place     60 minutes   \n",
       "720      https://en.wikipedia.org/wiki/Headline_Shooter  60 minutes[3]   \n",
       "1118   https://en.wikipedia.org/wiki/Escape_to_Paradise     60 minutes   \n",
       "158   https://en.wikipedia.org/wiki/Love_Never_Dies_...     60 minutes   \n",
       "1961    https://en.wikipedia.org/wiki/The_Blonde_Bandit     60 minutes   \n",
       "153        https://en.wikipedia.org/wiki/Heedless_Moths     60 minutes   \n",
       "5766  https://en.wikipedia.org/wiki/All_at_Sea_(1935...     60 minutes   \n",
       "143   https://en.wikipedia.org/wiki/A_Splendid_Hazar...     60 minutes   \n",
       "\n",
       "      Differrence  \n",
       "1274            0  \n",
       "722             0  \n",
       "169             0  \n",
       "720             0  \n",
       "1118            0  \n",
       "158             0  \n",
       "1961            0  \n",
       "153             0  \n",
       "5766            0  \n",
       "143             0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total search engines\n",
    "def search_engines():\n",
    "    q = input(\"Choose which engine to search[1,2 or 3](ex: 1): \\n\")\n",
    "    if q == '1':\n",
    "        print('** THE SIMPLE QUERY **\\n')\n",
    "        return query1()\n",
    "    if q == '2':\n",
    "        print('** THE QUERY WITH HIGHEST SIMILARITY **\\n')\n",
    "        return query2()\n",
    "    if q == '3':\n",
    "        print('** THE QUERY WITH HIGHEST SIMILARITY With DESIRED RUNTIME **\\n')\n",
    "        print('** The lower is the Differrence, the highest is the similarity **\\n')\n",
    "        return query3()\n",
    "    else:\n",
    "        print('** PLEASE TRY AGAIN WITH CHOOSING A SEARCH ENGINE **')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose which engine to search[1,2 or 3](ex: 1): \n",
      "3\n",
      "** THE QUERY WITH HIGHEST SIMILARITY With DESIRED RUNTIME **\n",
      "\n",
      "** The lower is the Differrence, the highest is the similarity **\n",
      "\n",
      "DESIRE RUNTIME(in mins): 90\n",
      "Enter The Query: Disney movies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "      <th>runtime</th>\n",
       "      <th>Differrence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sullivan's Travels</td>\n",
       "      <td>[\"Sullivan's Travels is a 1941 American comedy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sullivan%27s_Tra...</td>\n",
       "      <td>90 minutes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Condorman</td>\n",
       "      <td>[\"Condorman is a 1981 American adventure comed...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Condorman</td>\n",
       "      <td>90 minutes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Annie (1999 film)</td>\n",
       "      <td>['Annie is a 1999 American made-for-television...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Annie_(1999_film)</td>\n",
       "      <td>90 minutes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Benji the Hunted</td>\n",
       "      <td>[\"Benji the Hunted is a 1987 American children...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Benji_the_Hunted</td>\n",
       "      <td>89 minutes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Plunder &amp; Lightning</td>\n",
       "      <td>['Plunder &amp; Lightning is an animated televisio...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Plunder_%26_Ligh...</td>\n",
       "      <td>91 minutes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Castaway Cowboy</td>\n",
       "      <td>['The Castaway Cowboy is a 1974 American adven...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Castaway_Cowboy</td>\n",
       "      <td>91 minutes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Beverly Hills Chihuahua 3: Viva la Fiesta!</td>\n",
       "      <td>['Beverly Hills Chihuahua 3: Viva la Fiesta! i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Beverly_Hills_Ch...</td>\n",
       "      <td>89 minutes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>The Little Kidnappers (1990 film)</td>\n",
       "      <td>['The Little Kidnappers is a 1990 Canadian/Ame...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Little_Kidna...</td>\n",
       "      <td>92 minutes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Herbie Rides Again</td>\n",
       "      <td>['Herbie Rides Again is a 1974 American comedy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Herbie_Rides_Again</td>\n",
       "      <td>88 minutes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Tom and Huck</td>\n",
       "      <td>[\"Tom and Huck is a 1995 American adventure co...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Tom_and_Huck</td>\n",
       "      <td>92 minutes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "1                           Sullivan's Travels   \n",
       "22                                   Condorman   \n",
       "51                           Annie (1999 film)   \n",
       "27                            Benji the Hunted   \n",
       "35                         Plunder & Lightning   \n",
       "18                         The Castaway Cowboy   \n",
       "78  Beverly Hills Chihuahua 3: Viva la Fiesta!   \n",
       "90           The Little Kidnappers (1990 film)   \n",
       "19                          Herbie Rides Again   \n",
       "46                                Tom and Huck   \n",
       "\n",
       "                                                intro  \\\n",
       "1   [\"Sullivan's Travels is a 1941 American comedy...   \n",
       "22  [\"Condorman is a 1981 American adventure comed...   \n",
       "51  ['Annie is a 1999 American made-for-television...   \n",
       "27  [\"Benji the Hunted is a 1987 American children...   \n",
       "35  ['Plunder & Lightning is an animated televisio...   \n",
       "18  ['The Castaway Cowboy is a 1974 American adven...   \n",
       "78  ['Beverly Hills Chihuahua 3: Viva la Fiesta! i...   \n",
       "90  ['The Little Kidnappers is a 1990 Canadian/Ame...   \n",
       "19  ['Herbie Rides Again is a 1974 American comedy...   \n",
       "46  [\"Tom and Huck is a 1995 American adventure co...   \n",
       "\n",
       "                                                  url     runtime  Differrence  \n",
       "1   https://en.wikipedia.org/wiki/Sullivan%27s_Tra...  90 minutes            0  \n",
       "22            https://en.wikipedia.org/wiki/Condorman  90 minutes            0  \n",
       "51    https://en.wikipedia.org/wiki/Annie_(1999_film)  90 minutes            0  \n",
       "27     https://en.wikipedia.org/wiki/Benji_the_Hunted  89 minutes            1  \n",
       "35  https://en.wikipedia.org/wiki/Plunder_%26_Ligh...  91 minutes            1  \n",
       "18  https://en.wikipedia.org/wiki/The_Castaway_Cowboy  91 minutes            1  \n",
       "78  https://en.wikipedia.org/wiki/Beverly_Hills_Ch...  89 minutes            1  \n",
       "90  https://en.wikipedia.org/wiki/The_Little_Kidna...  92 minutes            2  \n",
       "19   https://en.wikipedia.org/wiki/Herbie_Rides_Again  88 minutes            2  \n",
       "46         https://en.wikipedia.org/wiki/Tom_and_Huck  92 minutes            2  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'a', 'i', 'n', 'i', 'a', 'a')\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "sequence = 'dataminingsapienza'\n",
    "\n",
    "count = set()\n",
    "done = False\n",
    "for i in range(len(sequence), 1, -1):\n",
    "    sub = list(combinations(sequence, i))\n",
    "    for k in range(len(sub)):\n",
    "        if sub[k] == sub[k][::-1]:\n",
    "            count.add(len(sub[k]))\n",
    "            print(sub[k])\n",
    "            done = True\n",
    "            break\n",
    "    if done: break\n",
    "\n",
    "print(max(count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
